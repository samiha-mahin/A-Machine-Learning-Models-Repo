{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOFwu0cORSD+ixWaeuaKXHX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/samiha-mahin/A-Machine-Learning-Models-Repo/blob/main/Confusion_Matrix.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## ‚úÖ What is a Confusion Matrix?\n",
        "\n",
        "A **confusion matrix** is a table that helps us **evaluate the performance** of a **classification model** by comparing **predicted values** with **actual values**.\n",
        "\n",
        "---\n",
        "\n",
        "### üî≤ Confusion Matrix Structure (Binary Classification)\n",
        "\n",
        "|                 | **Predicted: Yes**  | **Predicted: No**   |\n",
        "| --------------- | ------------------- | ------------------- |\n",
        "| **Actual: Yes** | True Positive (TP)  | False Negative (FN) |\n",
        "| **Actual: No**  | False Positive (FP) | True Negative (TN)  |\n",
        "\n",
        "---\n",
        "\n",
        "### üìò Example:\n",
        "\n",
        "Let‚Äôs say you built a model to predict whether an email is **spam**.\n",
        "\n",
        "Your test data has 100 emails, and your model gave the following result:\n",
        "\n",
        "|                          | **Spam (Predicted Yes)** | **Not Spam (Predicted No)** |\n",
        "| ------------------------ | ------------------------ | --------------------------- |\n",
        "| **Spam (Actual Yes)**    | 40 (TP)                  | 10 (FN)                     |\n",
        "| **Not Spam (Actual No)** | 5 (FP)                   | 45 (TN)                     |\n",
        "\n",
        "---\n",
        "\n",
        "## üìä Calculations:\n",
        "\n",
        "* **True Positive (TP) = 40** ‚Üí Model predicted spam and it really was spam\n",
        "* **False Negative (FN) = 10** ‚Üí Model said not spam, but it was spam\n",
        "* **False Positive (FP) = 5** ‚Üí Model said spam, but it was not\n",
        "* **True Negative (TN) = 45** ‚Üí Model predicted not spam and it was not spam\n",
        "\n",
        "---\n",
        "\n",
        "### üìà Evaluation Metrics:\n",
        "\n",
        "#### ‚úÖ **Accuracy** ‚Äì Overall how often the model is correct\n",
        "\n",
        "$$\n",
        "\\text{Accuracy} = \\frac{TP + TN}{TP + TN + FP + FN} = \\frac{40 + 45}{100} = 0.85 = 85\\%\n",
        "$$\n",
        "\n",
        "#### üéØ **Precision** ‚Äì How many predicted spams were actually spam?\n",
        "\n",
        "$$\n",
        "\\text{Precision} = \\frac{TP}{TP + FP} = \\frac{40}{40 + 5} = 0.89\n",
        "$$\n",
        "\n",
        "#### üîÅ **Recall** ‚Äì How many actual spams were detected?\n",
        "\n",
        "$$\n",
        "\\text{Recall} = \\frac{TP}{TP + FN} = \\frac{40}{40 + 10} = 0.80\n",
        "$$\n",
        "\n",
        "#### üéØ **F1 Score** ‚Äì Harmonic mean of precision and recall (balance)\n",
        "\n",
        "$$\n",
        "\\text{F1} = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}} = 2 \\times \\frac{0.89 \\times 0.80}{0.89 + 0.80} ‚âà 0.84\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ Summary Table\n",
        "\n",
        "| Metric    | Value | Meaning                                        |\n",
        "| --------- | ----- | ---------------------------------------------- |\n",
        "| Accuracy  | 85%   | Overall correct predictions                    |\n",
        "| Precision | 89%   | Correctly predicted spam from all spam guesses |\n",
        "| Recall    | 80%   | Found 80% of actual spam messages              |\n",
        "| F1 Score  | 84%   | Balance between precision and recall           |\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-1_pfrQHDXGp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M3M3-EVTwDH1"
      },
      "outputs": [],
      "source": []
    }
  ]
}