{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPwJJfj2c+bdrTD0O079W+v",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/samiha-mahin/A-Machine-Learning-Models-Repo/blob/main/PCA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### ðŸŒŸ What is PCA?\n",
        "\n",
        "**Principal Component Analysis (PCA)** is a technique used to **reduce the number of features (dimensions)** in your data **while keeping the most important information**.\n",
        "\n",
        "---\n",
        "\n",
        "### ðŸ“¦ Why do we need PCA?\n",
        "\n",
        "When your dataset has **too many features**, like:\n",
        "\n",
        "* 100+ columns (e.g., pixel values of an image),\n",
        "* or many measurements (e.g., in gene expression, climate data),\n",
        "\n",
        "It becomes:\n",
        "\n",
        "* Harder to **visualize**\n",
        "* Slower to **train ML models**\n",
        "* Risk of **overfitting**\n",
        "\n",
        "This problem is called the **Curse of Dimensionality**.\n",
        "\n",
        "---\n",
        "\n",
        "### ðŸ§  PCA helps by:\n",
        "\n",
        "* Finding **new axes (directions)** called **principal components**, where the **data spreads out the most**.\n",
        "* It then **rotates** and **projects** the data onto a smaller number of these axes (e.g., from 100D to 2D or 3D).\n",
        "* You keep only the **top few components** that explain most of the variation.\n",
        "\n",
        "---\n",
        "\n",
        "### ðŸ§ª Simple Example:\n",
        "\n",
        "Imagine you're studying students with:\n",
        "\n",
        "* Height\n",
        "* Weight\n",
        "* Shoe Size\n",
        "\n",
        "These are **3 features**. But they're all **related** â€” tall students often have higher weight and larger shoe size.\n",
        "\n",
        "PCA will:\n",
        "\n",
        "* Find one **principal component** that summarizes these 3 features (like \"overall body size\").\n",
        "* You can then use this 1 component instead of all 3.\n",
        "\n",
        "---\n",
        "\n",
        "### ðŸ“‰ Curse of Dimensionality Reduced:\n",
        "\n",
        "* **Before PCA**: 100 features â†’ high computation, overfitting\n",
        "* **After PCA**: Reduce to 10â€“20 components â†’ faster, less noise, better generalization\n",
        "\n",
        "PCA removes **redundant** or **less informative** features.\n",
        "\n",
        "---\n",
        "\n",
        "### ðŸŽ¯ Real-Life Example:\n",
        "\n",
        "In image compression:\n",
        "\n",
        "* A 64Ã—64 image = 4096 pixels (features!)\n",
        "* PCA can reduce this to just 100 components\n",
        "* You still get a recognizable image with **much less data**\n",
        "\n",
        "---\n",
        "\n",
        "### âœ… Summary\n",
        "\n",
        "| Term                        | Meaning                                                                              |\n",
        "| --------------------------- | ------------------------------------------------------------------------------------ |\n",
        "| **PCA**                     | Converts high-dimensional data into fewer dimensions keeping most of the information |\n",
        "| **Principal Components**    | New \"summary\" features that represent the data's variance                            |\n",
        "| **Curse of Dimensionality** | Problem of having too many features which makes ML harder                            |\n",
        "| **How PCA helps**           | Removes redundancy, reduces noise, simplifies models                                 |\n",
        "\n"
      ],
      "metadata": {
        "id": "inSYCSOtjCPb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JsYeYmwCTnz2"
      },
      "outputs": [],
      "source": []
    }
  ]
}