{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP7CNz90AwQXq31XZGkRugU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/samiha-mahin/A-Machine-Learning-Models-Repo/blob/main/Bagging_Ensemble.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### üå≥ What is Bagging?\n",
        "\n",
        "**Bagging** stands for **Bootstrap Aggregating**.\n",
        "\n",
        "It‚Äôs an **ensemble learning technique** that:\n",
        "\n",
        "* Combines the predictions of **multiple models** (usually of the same type).\n",
        "* Aims to **reduce variance** and **prevent overfitting**.\n",
        "* Works especially well with **unstable models** (like decision trees).\n",
        "\n",
        "---\n",
        "\n",
        "### üß† How Does It Work?\n",
        "\n",
        "1. **Bootstrapping (Sampling with replacement):**\n",
        "\n",
        "   * From the original dataset, you create **multiple new datasets** by randomly selecting data points **with replacement**.\n",
        "   * Each dataset is slightly different.\n",
        "\n",
        "2. **Train models:**\n",
        "\n",
        "   * You train a **separate model** on each of these datasets.\n",
        "   * Typically, the base model is a **Decision Tree**.\n",
        "\n",
        "3. **Aggregate the predictions:**\n",
        "\n",
        "   * For **classification**: Take the **majority vote**.\n",
        "   * For **regression**: Take the **average** of all outputs.\n",
        "\n",
        "---\n",
        "\n",
        "### üçé Real-Life Analogy\n",
        "\n",
        "Imagine you're trying to decide whether a new fruit is an **apple or a tomato**.\n",
        "\n",
        "* You ask **10 friends**, but give each of them a slightly different photo of the fruit (some brighter, some darker).\n",
        "* Each friend gives you their answer.\n",
        "* You **take the majority vote** to decide what the fruit is.\n",
        "\n",
        "‚Üí This is **bagging** in action ‚Äî using **multiple weak opinions** to make a **strong, stable decision**.\n",
        "\n",
        "---\n",
        "\n",
        "### üìä Proper Example: Predicting Loan Approval\n",
        "\n",
        "Suppose you're building a model to predict if a person should get a loan.\n",
        "\n",
        "#### Without Bagging:\n",
        "\n",
        "* You train **one decision tree**.\n",
        "* If the data changes a little, the tree might give **completely different results** (high variance).\n",
        "\n",
        "#### With Bagging:\n",
        "\n",
        "* You create 10 datasets from the original one (bootstrapped).\n",
        "* Train 10 decision trees (one on each dataset).\n",
        "* For a new applicant:\n",
        "\n",
        "  * Each tree makes a prediction (Yes or No).\n",
        "  * You take the **majority vote**.\n",
        "\n",
        "‚úÖ Result:\n",
        "\n",
        "* **More stable**\n",
        "* **Less likely to overfit**\n",
        "* **Higher accuracy**\n",
        "\n",
        "---\n",
        "\n",
        "### üì¶ Popular Bagging Example: **Random Forest**\n",
        "\n",
        "* Random Forest = Bagging of **Decision Trees**\n",
        "* It‚Äôs one of the most powerful and widely used ML algorithms.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ Benefits of Bagging\n",
        "\n",
        "| Feature            | Benefit                                |\n",
        "| ------------------ | -------------------------------------- |\n",
        "| Reduces Variance   | Makes unstable models more stable      |\n",
        "| Avoids Overfitting | Prevents one model from learning noise |\n",
        "| Easy to Implement  | Especially with decision trees         |\n",
        "| Improves Accuracy  | Better than a single model             |\n",
        "\n"
      ],
      "metadata": {
        "id": "r2wP9xXetXt9"
      }
    }
  ]
}