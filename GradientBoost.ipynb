{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPYcEcyVqRThNqo1tlzVKkO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/samiha-mahin/A-Machine-Learning-Models-Repo/blob/main/GradientBoost.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### üåü What is Gradient Boosting?\n",
        "\n",
        "**Gradient Boosting** is also an **ensemble method** like AdaBoost ‚Äî it combines many weak models (usually small decision trees) to create a **strong prediction model**.\n",
        "\n",
        "But instead of focusing on *which samples were wrong* like AdaBoost, Gradient Boosting tries to **reduce the errors step-by-step by learning the ‚Äúgradient‚Äù (direction) of mistakes** ‚Äî basically, it tries to fix the biggest errors gradually.\n",
        "\n",
        "---\n",
        "\n",
        "### üîÑ How Gradient Boosting Works (step-by-step, simply):\n",
        "\n",
        "---\n",
        "\n",
        "### üéØ Real-Life Example: Predicting House Prices\n",
        "\n",
        "Imagine you want to predict house prices based on some features.\n",
        "\n",
        "---\n",
        "\n",
        "#### Step 1: Start with a simple model\n",
        "\n",
        "* First, build a very simple model ‚Äî like predicting the **average price** for all houses.\n",
        "* It‚Äôs a weak prediction because it ignores differences between houses.\n",
        "\n",
        "---\n",
        "\n",
        "#### Step 2: Calculate errors (residuals)\n",
        "\n",
        "* Check how far off each prediction is from the actual price.\n",
        "* These differences are called **residuals** (actual - predicted).\n",
        "\n",
        "---\n",
        "\n",
        "#### Step 3: Build a new model to predict the errors\n",
        "\n",
        "* Next, build a new weak model that tries to predict those residuals.\n",
        "* This model‚Äôs job is to correct the mistakes of the first model.\n",
        "\n",
        "---\n",
        "\n",
        "#### Step 4: Update the prediction\n",
        "\n",
        "* Add the predictions from the new model to the old predictions.\n",
        "* The overall prediction improves a bit.\n",
        "\n",
        "---\n",
        "\n",
        "#### Step 5: Repeat many times\n",
        "\n",
        "* Keep repeating Steps 2‚Äì4:\n",
        "\n",
        "  * Calculate new residuals (errors) from the updated predictions.\n",
        "  * Build another model to predict these new residuals.\n",
        "  * Add the new model‚Äôs predictions to the total prediction.\n",
        "\n",
        "---\n",
        "\n",
        "### üì¶ Final result:\n",
        "\n",
        "After many rounds, all these small corrections add up to a **strong model** that accurately predicts house prices.\n",
        "\n",
        "---\n",
        "\n",
        "### üí° Key Concepts:\n",
        "\n",
        "| Concept            | Explanation                                                                           |\n",
        "| ------------------ | ------------------------------------------------------------------------------------- |\n",
        "| **Weak learner**   | Usually small decision trees (simple models)                                          |\n",
        "| **Residuals**      | Errors of previous model‚Äôs prediction                                                 |\n",
        "| **Additive model** | Each new model fixes errors of previous models                                        |\n",
        "| **Gradient**       | Direction of steepest error reduction (like slope)                                    |\n",
        "| **Learning rate**  | How much of the new model‚Äôs prediction is added each time (helps control overfitting) |\n",
        "\n",
        "---\n",
        "\n",
        "### üî• Why Gradient Boosting is Powerful:\n",
        "\n",
        "* It **optimizes errors step-by-step** in the direction that reduces error fastest (using gradient descent ideas).\n",
        "* Flexible and works well on many problems (classification, regression).\n",
        "* Can handle different loss functions (e.g., squared error, absolute error).\n",
        "* Usually more accurate than AdaBoost, but more complex.\n",
        "\n",
        "---\n",
        "\n",
        "### üö´ Limitations:\n",
        "\n",
        "* Can be **slow to train** because it builds models sequentially.\n",
        "* Can **overfit** if too many iterations or too complex trees are used.\n",
        "* Needs careful tuning of parameters (like learning rate, number of trees).\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ Summary (simple checklist):\n",
        "\n",
        "* \\[‚úì] Builds model step-by-step to fix errors (residuals)\n",
        "* \\[‚úì] Uses gradient (error direction) to improve predictions\n",
        "* \\[‚úì] Combines many weak learners additively\n",
        "* \\[‚úì] Controls overfitting with learning rate and tree size\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "xIZERVWlrm3U"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q1lSKslZrKPH"
      },
      "outputs": [],
      "source": []
    }
  ]
}